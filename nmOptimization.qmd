# Optimization {#sec-optim}

> *It is not enough to do your best; you must know what to do, and then do your best.*\
> --[W. Edwards Deming](https://en.wikipedia.org/wiki/W._Edwards_Deming)

In applied mathematics we are often not interested in all solutions of a problem but in the optimal solution. Optimization therefore permeates many areas of mathematics and science. In this section we will look at a few examples of optimization problems and the numerical methods that can be used to solve them.

------------------------------------------------------------------------

::: {#exr-3.48}
Here is an atypically easy optimisation problem that you can quickly do by hand:

 A piece of cardboard measuring 20cm by 20cm is to be cut so that it can be folded into a box without a lid (see @fig-3.8). We want to find the size of the cut, $x$, that maximizes the volume of the box.

1.  Write a function for the volume of the box resulting from a cut of size $x$. What is the domain of your function?

2.  We know that we want to maximize this function so go through the full Calculus exercise to find the maximum:

    -   take the derivative

    -   set it to zero

    -   find the critical points

    -   test the critical points and the boundaries of the domain using the extreme value theorem to determine the $x$ that gives the maximum.

![Folds to make a cardboard box](figures/Optimization/CardboardBox.png){#fig-3.8 alt="Folds to make a cardboard box" height=6cm}
:::

------------------------------------------------------------------------

An optimization problem is approached by first writing the quantity you want to optimize as a function of the parameters of the model. 
In the previous exercise that was the function $V(x)$ that gives the volume of the box as a function of the parameter $x$, which was the length of the cut. That function then needs to be maximized (or minimized, depending on what is optimal). 

------------------------------------------------------------------------

::: {#exr-3.49c}

In the previous example it was easy to find the value of $x$ that maximized the function analytically However, in many cases it is not so easy. The equation for the parameters that arises from setting the derivatives to zero is usually not solvable analytically. In these cases we need to use numerical methods to find the extremum. Take for example the function 
$$f(x) = e^{-x^2} + \sin(x^2)$$ 
on the domain $0 \le x \le 1.5$. The maximum of this function on this domain can not be determined analytically.

Use Python to make a plot of this function over this domain. You should get something similar to the graph shown in @fig-opt-1. What is the $x$ that maximizes the function on this domain? What is the $x$ that minimizes the function on this domain?

```{python}
#| label: fig-opt-1
#| fig-cap: Graph of the function $f(x) = e^{-x^2} + \sin(x^2)$.
#| fig-alt: Graph of example function
#| column: page-right
#| code-fold: true
import numpy as np
import matplotlib.pyplot as plt
x = np.linspace(0,1.5,100)
f = np.exp(-x**2) + np.sin(x**2)
plt.xlabel('$x$')
plt.ylabel('$f(x)$')
plt.plot(x,f)
```
:::

------------------------------------------------------------------------

The intuition behind numerical optimization schemes is typically to visualize the function that you are trying to minimize or maximize and think about either climbing the hill to the top (maximization) or descending the hill to the bottom (minimization).

------------------------------------------------------------------------

::: {#exr-3.49}
If you were blind folded and standing on a hillside, could you find the top of the hill? (assume no trees and no cliffs ...this is not supposed to be dangerous) How would you do it? Explain your technique clearly.
:::

------------------------------------------------------------------------

::: {#exr-3.50}
 If you were blind folded and standing on a crater on the moon could you find the lowest point? How would you do it? Remember that you can hop as far as you like ... because gravity ... but sometimes that's not a great thing because you could hop too far.
:::

------------------------------------------------------------------------

Clearly there is no difference between finding the maximum of a function and finding the minimum of a function. The maximum of a function $f$ is exactly at the same point as the minimum of the function $-f$. For concreteness we will from now on focus on finding the minimum of a function.

## Single Variable Optimization {#sec-1D-optimization}

The preceding thought exercises have given you intuition about finding extrema in a two-dimensional landscape. But first we will reduce back to one-dimensional optimization problems before generalising to multiple dimensions in the next section.

::: {#exr-3.51}
Try to turn your intuitions into algorithms. If $f(x)$ is the function that you are trying to maximize then turn your ideas from the previous exercises into step-by-step algorithms which could be coded. Then try out your codes on the function 
\begin{equation}
f(x) = -e^{-x^2} - \sin(x^2)
\end{equation}
 to see if your algorithms can find the local minimum near $x \approx 1.14$. Try to generate several different algorithms.

:::

------------------------------------------------------------------------

One obvious method would be to simply evaluate the function at many points and choose the smallest value. This is called a *brute force* search.

``` python
import numpy as np
x = np.linspace(0,1.5,1000)
f = np.exp(-x**2) + np.sin(x**2)
print(x[np.argmin(f)])
```
This method is not very efficient. Just think about how often you would need to evaluate the function for the above approach to give the answer to 12 decimal places. It would be a lot! Your method should be more efficient.

The advantage of this brute force method is that it is guaranteed to find the global minimum in the interval. Any other, more efficient method can get stuck in local minima. 

------------------------------------------------------------------------

::: {#exr-3.51b}
Here is an idea for a method that is similar to the bisection method for root finding. 

In the bisection method we needed a starting interval so that the function values had opposite signs at the endpoints. You were therefore guaranteed that there would be at least one root in that interval. Then you chose a point in the middle of the interval and by looking at the function value at that new point were able to choose an appropriate smaller interval that was still guaranteed to contain a root. By repeating this you honed in on the root.

Unfortunately by just looking at the function values at two point there is no way of knowing whether there is a minimum between them. However, if you were to look at the function values at three points and found that the value at the middle point was less than the values at the endpoints then you would know that there was a minimum between the endpoints.

The idea now is to choose a new point between the two outer points, compare the function value there to those at the previous three points, and then choose a new triplet of points that is guaranteed to contain a minimum. By repeating this process you would hone in on the minimum.

Complete the following function to implement this idea. You need to think about how to choose the new point and then how to choose the new triplet. 

``` python
def bracketing(f, a, b, c, tol = 1e-12):
    """
    Find an approximation of a local minimum of a function f within the 
    interval [a, b] using a bracketing method.

    The function narrows down the interval [a, b] by maintaining a 
    triplet (a, c, b) where f(c) < f(a) and f(c) < f(b).
    The process iteratively updates the triplet to home in on the minimum, 
    stopping when the interval is smaller than `tol`.

    Parameters:
    f (function): A unimodal function to minimize.
    a, b (float): The initial interval bounds where the minimum is to be 
                  searched. It is assumed that a < b.
    c (float): An initial point within the interval (a, b) where 
               f(c) < f(a) and f(c) < f(b).
    tol (float): The tolerance for the convergence of the algorithm. 
                 The function stops when b - a < tol.

    Returns:
    float: An approximation of a point where f achieves a local minimum.
    """

    # Check that the point are ordered a < c < b

    # Check that the function value at `c` is lower than at both `a` and `b`

    # Loop until you have an interval smaller than the tolerance
    while b-a > tol:

        # Choose a new point `d` between `a` and `b`
        # Think about what is the most efficient choice

        # Compare f(d) with f(a), f(b) and f(c) in various ways and use
        # that to choose a new triplet `a`, `b`, `c` in such a way that
        # b-a has decreased but f(c) is still lower than both f(a) and f(b)

        # While debugging, include a print statement to let you know what
        # is happening within your loop
    
    # Now `c` is guaranteed to be within `tol` of the true minimum
    return c
    
```

:::

------------------------------------------------------------------------


